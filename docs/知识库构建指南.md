# 🧠 PeerPortal 知识库构建指南

## 📋 概述

本指南详细说明如何使用 Chroma 云数据库为 PeerPortal AI Agent 构建高效的知识库系统，支持文档摄取、语义搜索和智能问答。

## 🎯 系统特性

### ✅ 核心功能
- **多格式支持**: PDF、Word、TXT、Markdown、HTML
- **智能分块**: 自动文本分割和语义块管理
- **向量搜索**: 基于语义相似度的精准检索
- **多租户支持**: 按租户隔离数据，支持多用户场景
- **容错设计**: 优雅降级，确保系统稳定性

### 🚀 技术优势
- **云端存储**: 使用 Chroma 云数据库，无需自建向量数据库
- **本地嵌入**: 支持 Sentence Transformers，无需依赖外部 API
- **异步处理**: 高性能异步文档处理
- **灵活配置**: 支持多种嵌入模型和参数调优

## 🔧 环境配置

### 1. 安装依赖

```bash
# 核心依赖
pip install chromadb

# 文档处理依赖
pip install PyPDF2 python-docx beautifulsoup4 markdown

# 嵌入模型依赖（可选）
pip install sentence-transformers

# OpenAI 支持（可选）
pip install openai
```

### 2. Chroma 云数据库配置

```python
# 你的 Chroma 配置信息
CHROMA_CONFIG = {
    "api_key": "ck-EoDTZTCRe9Qb3LaWGEQA2EGDXoqx5FmZ93Y2KGSfQniL",
    "tenant": "fd1cb388-55f9-432c-9fc3-b12811c67ee0", 
    "database": "test-global-cs"
}
```

## 🚀 快速开始

### 方法一：使用管理工具

```bash
# 进入后端目录
cd backend

# 查看知识库信息
python knowledge_base_manager.py info

# 添加单个文档
python knowledge_base_manager.py add /path/to/document.pdf

# 批量添加目录
python knowledge_base_manager.py add /path/to/documents/ --recursive

# 搜索文档
python knowledge_base_manager.py search "如何申请留学？"

# 进入交互模式
python knowledge_base_manager.py interactive
```

### 方法二：编程接口

```python
from libs.knowledge_base.chroma_knowledge_base import initialize_knowledge_base

# 初始化知识库
kb = initialize_knowledge_base(
    api_key="your-api-key",
    tenant="your-tenant", 
    database="your-database",
    collection_name="documents"
)

# 添加文档
result = await kb.add_document(
    file_path="document.pdf",
    tenant_id="user_123",
    metadata={"category": "留学指南"}
)

# 搜索文档
results = await kb.search(
    query="GPA计算方法",
    top_k=5,
    tenant_id="user_123"
)
```

## 📚 详细使用教程

### 1. 文档添加

#### 支持的文件格式

| 格式 | 扩展名 | 处理方式 |
|------|--------|----------|
| PDF | .pdf | PyPDF2 提取文本 |
| Word | .docx, .doc | python-docx 解析 |
| 文本 | .txt | 直接读取 |
| Markdown | .md | 转换为纯文本 |
| HTML | .html, .htm | BeautifulSoup 提取 |

#### 文档处理流程

```python
# 1. 文档加载
document = await loader.load_file("/path/to/file.pdf")

# 2. 智能分块
chunks = text_splitter.split_document(document)

# 3. 生成嵌入
embeddings = await embedding_provider.embed_texts([chunk.content for chunk in chunks])

# 4. 存储到 Chroma
collection.add(
    ids=[chunk.id for chunk in chunks],
    documents=[chunk.content for chunk in chunks], 
    embeddings=embeddings,
    metadatas=[chunk.metadata for chunk in chunks]
)
```

### 2. 智能搜索

#### 搜索参数

```python
results = await kb.search(
    query="用户查询",           # 搜索查询
    top_k=5,                   # 返回结果数量
    tenant_id="user_123",      # 租户过滤
    filter_metadata={          # 元数据过滤
        "category": "留学指南",
        "language": "中文"
    }
)
```

#### 结果格式

```python
{
    "content": "文档块内容...",
    "metadata": {
        "filename": "留学申请指南.pdf",
        "chunk_index": 2,
        "total_chunks": 10,
        "category": "留学指南"
    },
    "distance": 0.15,          # 向量距离
    "score": 0.85              # 相似度分数
}
```

### 3. 与 Agent 系统集成

#### 在 Agent 中使用知识库

```python
from libs.knowledge_base.chroma_knowledge_base import get_knowledge_base

class StudyPlannerAgent:
    def __init__(self):
        self.kb = get_knowledge_base()
    
    async def answer_question(self, user_query: str, user_id: str):
        # 1. 搜索相关知识
        knowledge = await self.kb.search(
            query=user_query,
            top_k=3,
            tenant_id=user_id
        )
        
        # 2. 构建上下文
        context = "\\n".join([
            f"参考资料: {result['content'][:500]}..."
            for result in knowledge
        ])
        
        # 3. 生成回答
        messages = [
            {"role": "system", "content": "你是留学规划助手，基于提供的资料回答问题。"},
            {"role": "user", "content": f"问题: {user_query}\\n\\n参考资料:\\n{context}"}
        ]
        
        response = await self.llm_manager.chat(messages=messages)
        return response.content
```

## 🔧 高级配置

### 1. 文本分块策略

```python
# 自定义分块参数
text_splitter = TextSplitter(
    chunk_size=1000,      # 块大小
    chunk_overlap=200     # 重叠大小
)

# 基于段落分割，保持语义完整性
chunks = text_splitter.split_document(document)
```

### 2. 嵌入模型选择

```python
# 使用 Sentence Transformers（推荐）
embedding_provider = EmbeddingProvider(
    provider="sentence_transformers",
    model_name="paraphrase-multilingual-MiniLM-L12-v2"  # 支持中文
)

# 使用 OpenAI（需要 API Key）
embedding_provider = EmbeddingProvider(
    provider="openai",
    model_name="text-embedding-ada-002"
)
```

### 3. 多租户数据隔离

```python
# 添加文档时指定租户
await kb.add_document(
    file_path="document.pdf",
    tenant_id="company_A"  # 租户标识
)

# 搜索时按租户过滤
results = await kb.search(
    query="查询内容",
    tenant_id="company_A"  # 只搜索该租户的数据
)
```

## 📊 性能优化

### 1. 批量处理

```python
# 批量添加文档
async def batch_add_documents(file_paths: List[str], tenant_id: str):
    tasks = []
    for file_path in file_paths:
        task = kb.add_document(file_path, tenant_id=tenant_id)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

### 2. 缓存策略

```python
# 嵌入向量缓存
import hashlib
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_text_hash(text: str) -> str:
    return hashlib.md5(text.encode()).hexdigest()

# 查询结果缓存
query_cache = {}

async def cached_search(query: str, **kwargs):
    cache_key = f"{query}_{kwargs}"
    if cache_key in query_cache:
        return query_cache[cache_key]
    
    results = await kb.search(query, **kwargs)
    query_cache[cache_key] = results
    return results
```

### 3. 监控指标

```python
import time
import logging

class PerformanceMonitor:
    def __init__(self):
        self.logger = logging.getLogger("performance")
    
    async def monitor_search(self, query: str, **kwargs):
        start_time = time.time()
        results = await kb.search(query, **kwargs)
        duration = time.time() - start_time
        
        self.logger.info(f"搜索耗时: {duration:.3f}s, 查询: {query}, 结果数: {len(results)}")
        return results
```

## 🧪 测试验证

### 运行测试脚本

```bash
# 运行完整测试
python test_knowledge_base.py

# 测试特定功能
python -c "
import asyncio
from test_knowledge_base import test_knowledge_base
asyncio.run(test_knowledge_base())
"
```

### 测试覆盖范围

- ✅ 文档加载和处理
- ✅ 文本分块和嵌入
- ✅ 向量存储和检索
- ✅ 多租户数据隔离
- ✅ 错误处理和容错
- ✅ 性能基准测试

## 🔍 故障排查

### 1. 连接问题

```python
# 检查 Chroma 连接
try:
    client = chromadb.CloudClient(
        api_key=API_KEY,
        tenant=TENANT,
        database=DATABASE
    )
    collections = client.list_collections()
    print(f"✅ 连接成功，集合数量: {len(collections)}")
except Exception as e:
    print(f"❌ 连接失败: {e}")
```

### 2. 嵌入模型问题

```python
# 检查模型加载
from sentence_transformers import SentenceTransformer

try:
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    test_embedding = model.encode(["测试文本"])
    print(f"✅ 模型加载成功，向量维度: {len(test_embedding[0])}")
except Exception as e:
    print(f"❌ 模型加载失败: {e}")
```

### 3. 文档处理问题

```python
# 检查文档加载
from libs.knowledge_base.chroma_knowledge_base import DocumentLoader

loader = DocumentLoader()
try:
    doc = await loader.load_file("test.pdf")
    print(f"✅ 文档加载成功，内容长度: {len(doc.content)}")
except Exception as e:
    print(f"❌ 文档加载失败: {e}")
```

## 📈 扩展功能

### 1. 自定义文档处理器

```python
class CustomLoader(BaseLoader):
    async def load(self, file_path: str) -> List[DocumentChunk]:
        # 自定义加载逻辑
        pass

# 注册自定义加载器
LoaderFactory.register_loader(".custom", CustomLoader)
```

### 2. 高级搜索功能

```python
# 混合搜索（向量 + 关键词）
async def hybrid_search(query: str, **kwargs):
    # 向量搜索
    vector_results = await kb.search(query, **kwargs)
    
    # 关键词搜索
    keyword_results = await keyword_search(query, **kwargs)
    
    # 结果融合
    combined_results = merge_search_results(vector_results, keyword_results)
    return combined_results
```

### 3. 实时更新

```python
# 文档变更监听
import watchdog
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class DocumentWatcher(FileSystemEventHandler):
    def on_modified(self, event):
        if not event.is_directory:
            asyncio.create_task(kb.update_document(event.src_path))

# 启动文档监听
observer = Observer()
observer.schedule(DocumentWatcher(), "/path/to/documents", recursive=True)
observer.start()
```

## 📚 最佳实践

### 1. 文档组织

```
documents/
├── 留学指南/
│   ├── 申请流程.pdf
│   ├── GPA计算.md
│   └── 推荐信模板.docx
├── 学校信息/
│   ├── 美国大学排名.txt
│   └── 专业介绍.html
└── 政策法规/
    ├── 签证政策.pdf
    └── 移民指南.md
```

### 2. 元数据设计

```python
metadata = {
    "category": "留学指南",      # 文档分类
    "language": "中文",         # 语言标识  
    "source": "官方网站",       # 来源信息
    "version": "2024.1",       # 版本号
    "tags": ["GPA", "申请"],    # 标签
    "author": "留学专家",       # 作者
    "created_date": "2024-01-01",  # 创建日期
    "tenant_id": "user_123"    # 租户ID
}
```

### 3. 查询优化

```python
# 查询预处理
def preprocess_query(query: str) -> str:
    # 去除停用词
    # 同义词扩展
    # 拼写纠正
    return processed_query

# 结果后处理
def postprocess_results(results: List[Dict]) -> List[Dict]:
    # 去重
    # 重排序
    # 质量过滤
    return filtered_results
```

## 🎯 实际应用场景

### 1. 留学咨询 Agent

```python
# 专门的留学咨询知识库
study_kb = ChromaKnowledgeBase(
    collection_name="study_abroad_guide",
    embedding_provider="sentence_transformers"
)

# 添加留学相关文档
await study_kb.add_document("留学申请指南.pdf", metadata={"type": "guide"})
await study_kb.add_document("GPA计算方法.md", metadata={"type": "reference"})
```

### 2. 多语言支持

```python
# 中文文档
await kb.add_document("中文指南.pdf", metadata={"language": "zh"})

# 英文文档  
await kb.add_document("english_guide.pdf", metadata={"language": "en"})

# 按语言搜索
results = await kb.search("申请流程", filter_metadata={"language": "zh"})
```

### 3. 版本管理

```python
# 添加版本化文档
await kb.add_document(
    "政策文件.pdf",
    metadata={
        "version": "2024.1",
        "effective_date": "2024-01-01",
        "status": "current"
    }
)

# 查询最新版本
results = await kb.search(
    "签证政策",
    filter_metadata={"status": "current"}
)
```

---

💡 **提示**: 知识库是 AI Agent 的重要组成部分，建议在实际使用中根据具体需求调整配置参数和处理策略。

