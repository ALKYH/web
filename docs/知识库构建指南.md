# ğŸ§  PeerPortal çŸ¥è¯†åº“æ„å»ºæŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•ä½¿ç”¨ Chroma äº‘æ•°æ®åº“ä¸º PeerPortal AI Agent æ„å»ºé«˜æ•ˆçš„çŸ¥è¯†åº“ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æ¡£æ‘„å–ã€è¯­ä¹‰æœç´¢å’Œæ™ºèƒ½é—®ç­”ã€‚

## ğŸ¯ ç³»ç»Ÿç‰¹æ€§

### âœ… æ ¸å¿ƒåŠŸèƒ½
- **å¤šæ ¼å¼æ”¯æŒ**: PDFã€Wordã€TXTã€Markdownã€HTML
- **æ™ºèƒ½åˆ†å—**: è‡ªåŠ¨æ–‡æœ¬åˆ†å‰²å’Œè¯­ä¹‰å—ç®¡ç†
- **å‘é‡æœç´¢**: åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„ç²¾å‡†æ£€ç´¢
- **å¤šç§Ÿæˆ·æ”¯æŒ**: æŒ‰ç§Ÿæˆ·éš”ç¦»æ•°æ®ï¼Œæ”¯æŒå¤šç”¨æˆ·åœºæ™¯
- **å®¹é”™è®¾è®¡**: ä¼˜é›…é™çº§ï¼Œç¡®ä¿ç³»ç»Ÿç¨³å®šæ€§

### ğŸš€ æŠ€æœ¯ä¼˜åŠ¿
- **äº‘ç«¯å­˜å‚¨**: ä½¿ç”¨ Chroma äº‘æ•°æ®åº“ï¼Œæ— éœ€è‡ªå»ºå‘é‡æ•°æ®åº“
- **æœ¬åœ°åµŒå…¥**: æ”¯æŒ Sentence Transformersï¼Œæ— éœ€ä¾èµ–å¤–éƒ¨ API
- **å¼‚æ­¥å¤„ç†**: é«˜æ€§èƒ½å¼‚æ­¥æ–‡æ¡£å¤„ç†
- **çµæ´»é…ç½®**: æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹å’Œå‚æ•°è°ƒä¼˜

## ğŸ”§ ç¯å¢ƒé…ç½®

### 1. å®‰è£…ä¾èµ–

```bash
# æ ¸å¿ƒä¾èµ–
pip install chromadb

# æ–‡æ¡£å¤„ç†ä¾èµ–
pip install PyPDF2 python-docx beautifulsoup4 markdown

# åµŒå…¥æ¨¡å‹ä¾èµ–ï¼ˆå¯é€‰ï¼‰
pip install sentence-transformers

# OpenAI æ”¯æŒï¼ˆå¯é€‰ï¼‰
pip install openai
```

### 2. Chroma äº‘æ•°æ®åº“é…ç½®

```python
# ä½ çš„ Chroma é…ç½®ä¿¡æ¯
CHROMA_CONFIG = {
    "api_key": "ck-EoDTZTCRe9Qb3LaWGEQA2EGDXoqx5FmZ93Y2KGSfQniL",
    "tenant": "fd1cb388-55f9-432c-9fc3-b12811c67ee0", 
    "database": "test-global-cs"
}
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³•ä¸€ï¼šä½¿ç”¨ç®¡ç†å·¥å…·

```bash
# è¿›å…¥åç«¯ç›®å½•
cd backend

# æŸ¥çœ‹çŸ¥è¯†åº“ä¿¡æ¯
python knowledge_base_manager.py info

# æ·»åŠ å•ä¸ªæ–‡æ¡£
python knowledge_base_manager.py add /path/to/document.pdf

# æ‰¹é‡æ·»åŠ ç›®å½•
python knowledge_base_manager.py add /path/to/documents/ --recursive

# æœç´¢æ–‡æ¡£
python knowledge_base_manager.py search "å¦‚ä½•ç”³è¯·ç•™å­¦ï¼Ÿ"

# è¿›å…¥äº¤äº’æ¨¡å¼
python knowledge_base_manager.py interactive
```

### æ–¹æ³•äºŒï¼šç¼–ç¨‹æ¥å£

```python
from libs.knowledge_base.chroma_knowledge_base import initialize_knowledge_base

# åˆå§‹åŒ–çŸ¥è¯†åº“
kb = initialize_knowledge_base(
    api_key="your-api-key",
    tenant="your-tenant", 
    database="your-database",
    collection_name="documents"
)

# æ·»åŠ æ–‡æ¡£
result = await kb.add_document(
    file_path="document.pdf",
    tenant_id="user_123",
    metadata={"category": "ç•™å­¦æŒ‡å—"}
)

# æœç´¢æ–‡æ¡£
results = await kb.search(
    query="GPAè®¡ç®—æ–¹æ³•",
    top_k=5,
    tenant_id="user_123"
)
```

## ğŸ“š è¯¦ç»†ä½¿ç”¨æ•™ç¨‹

### 1. æ–‡æ¡£æ·»åŠ 

#### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

| æ ¼å¼ | æ‰©å±•å | å¤„ç†æ–¹å¼ |
|------|--------|----------|
| PDF | .pdf | PyPDF2 æå–æ–‡æœ¬ |
| Word | .docx, .doc | python-docx è§£æ |
| æ–‡æœ¬ | .txt | ç›´æ¥è¯»å– |
| Markdown | .md | è½¬æ¢ä¸ºçº¯æ–‡æœ¬ |
| HTML | .html, .htm | BeautifulSoup æå– |

#### æ–‡æ¡£å¤„ç†æµç¨‹

```python
# 1. æ–‡æ¡£åŠ è½½
document = await loader.load_file("/path/to/file.pdf")

# 2. æ™ºèƒ½åˆ†å—
chunks = text_splitter.split_document(document)

# 3. ç”ŸæˆåµŒå…¥
embeddings = await embedding_provider.embed_texts([chunk.content for chunk in chunks])

# 4. å­˜å‚¨åˆ° Chroma
collection.add(
    ids=[chunk.id for chunk in chunks],
    documents=[chunk.content for chunk in chunks], 
    embeddings=embeddings,
    metadatas=[chunk.metadata for chunk in chunks]
)
```

### 2. æ™ºèƒ½æœç´¢

#### æœç´¢å‚æ•°

```python
results = await kb.search(
    query="ç”¨æˆ·æŸ¥è¯¢",           # æœç´¢æŸ¥è¯¢
    top_k=5,                   # è¿”å›ç»“æœæ•°é‡
    tenant_id="user_123",      # ç§Ÿæˆ·è¿‡æ»¤
    filter_metadata={          # å…ƒæ•°æ®è¿‡æ»¤
        "category": "ç•™å­¦æŒ‡å—",
        "language": "ä¸­æ–‡"
    }
)
```

#### ç»“æœæ ¼å¼

```python
{
    "content": "æ–‡æ¡£å—å†…å®¹...",
    "metadata": {
        "filename": "ç•™å­¦ç”³è¯·æŒ‡å—.pdf",
        "chunk_index": 2,
        "total_chunks": 10,
        "category": "ç•™å­¦æŒ‡å—"
    },
    "distance": 0.15,          # å‘é‡è·ç¦»
    "score": 0.85              # ç›¸ä¼¼åº¦åˆ†æ•°
}
```

### 3. ä¸ Agent ç³»ç»Ÿé›†æˆ

#### åœ¨ Agent ä¸­ä½¿ç”¨çŸ¥è¯†åº“

```python
from libs.knowledge_base.chroma_knowledge_base import get_knowledge_base

class StudyPlannerAgent:
    def __init__(self):
        self.kb = get_knowledge_base()
    
    async def answer_question(self, user_query: str, user_id: str):
        # 1. æœç´¢ç›¸å…³çŸ¥è¯†
        knowledge = await self.kb.search(
            query=user_query,
            top_k=3,
            tenant_id=user_id
        )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\\n".join([
            f"å‚è€ƒèµ„æ–™: {result['content'][:500]}..."
            for result in knowledge
        ])
        
        # 3. ç”Ÿæˆå›ç­”
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ç•™å­¦è§„åˆ’åŠ©æ‰‹ï¼ŒåŸºäºæä¾›çš„èµ„æ–™å›ç­”é—®é¢˜ã€‚"},
            {"role": "user", "content": f"é—®é¢˜: {user_query}\\n\\nå‚è€ƒèµ„æ–™:\\n{context}"}
        ]
        
        response = await self.llm_manager.chat(messages=messages)
        return response.content
```

## ğŸ”§ é«˜çº§é…ç½®

### 1. æ–‡æœ¬åˆ†å—ç­–ç•¥

```python
# è‡ªå®šä¹‰åˆ†å—å‚æ•°
text_splitter = TextSplitter(
    chunk_size=1000,      # å—å¤§å°
    chunk_overlap=200     # é‡å å¤§å°
)

# åŸºäºæ®µè½åˆ†å‰²ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
chunks = text_splitter.split_document(document)
```

### 2. åµŒå…¥æ¨¡å‹é€‰æ‹©

```python
# ä½¿ç”¨ Sentence Transformersï¼ˆæ¨èï¼‰
embedding_provider = EmbeddingProvider(
    provider="sentence_transformers",
    model_name="paraphrase-multilingual-MiniLM-L12-v2"  # æ”¯æŒä¸­æ–‡
)

# ä½¿ç”¨ OpenAIï¼ˆéœ€è¦ API Keyï¼‰
embedding_provider = EmbeddingProvider(
    provider="openai",
    model_name="text-embedding-ada-002"
)
```

### 3. å¤šç§Ÿæˆ·æ•°æ®éš”ç¦»

```python
# æ·»åŠ æ–‡æ¡£æ—¶æŒ‡å®šç§Ÿæˆ·
await kb.add_document(
    file_path="document.pdf",
    tenant_id="company_A"  # ç§Ÿæˆ·æ ‡è¯†
)

# æœç´¢æ—¶æŒ‰ç§Ÿæˆ·è¿‡æ»¤
results = await kb.search(
    query="æŸ¥è¯¢å†…å®¹",
    tenant_id="company_A"  # åªæœç´¢è¯¥ç§Ÿæˆ·çš„æ•°æ®
)
```

## ğŸ“Š æ€§èƒ½ä¼˜åŒ–

### 1. æ‰¹é‡å¤„ç†

```python
# æ‰¹é‡æ·»åŠ æ–‡æ¡£
async def batch_add_documents(file_paths: List[str], tenant_id: str):
    tasks = []
    for file_path in file_paths:
        task = kb.add_document(file_path, tenant_id=tenant_id)
        tasks.append(task)
    
    results = await asyncio.gather(*tasks)
    return results
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# åµŒå…¥å‘é‡ç¼“å­˜
import hashlib
from functools import lru_cache

@lru_cache(maxsize=1000)
def get_text_hash(text: str) -> str:
    return hashlib.md5(text.encode()).hexdigest()

# æŸ¥è¯¢ç»“æœç¼“å­˜
query_cache = {}

async def cached_search(query: str, **kwargs):
    cache_key = f"{query}_{kwargs}"
    if cache_key in query_cache:
        return query_cache[cache_key]
    
    results = await kb.search(query, **kwargs)
    query_cache[cache_key] = results
    return results
```

### 3. ç›‘æ§æŒ‡æ ‡

```python
import time
import logging

class PerformanceMonitor:
    def __init__(self):
        self.logger = logging.getLogger("performance")
    
    async def monitor_search(self, query: str, **kwargs):
        start_time = time.time()
        results = await kb.search(query, **kwargs)
        duration = time.time() - start_time
        
        self.logger.info(f"æœç´¢è€—æ—¶: {duration:.3f}s, æŸ¥è¯¢: {query}, ç»“æœæ•°: {len(results)}")
        return results
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
# è¿è¡Œå®Œæ•´æµ‹è¯•
python test_knowledge_base.py

# æµ‹è¯•ç‰¹å®šåŠŸèƒ½
python -c "
import asyncio
from test_knowledge_base import test_knowledge_base
asyncio.run(test_knowledge_base())
"
```

### æµ‹è¯•è¦†ç›–èŒƒå›´

- âœ… æ–‡æ¡£åŠ è½½å’Œå¤„ç†
- âœ… æ–‡æœ¬åˆ†å—å’ŒåµŒå…¥
- âœ… å‘é‡å­˜å‚¨å’Œæ£€ç´¢
- âœ… å¤šç§Ÿæˆ·æ•°æ®éš”ç¦»
- âœ… é”™è¯¯å¤„ç†å’Œå®¹é”™
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•

## ğŸ” æ•…éšœæ’æŸ¥

### 1. è¿æ¥é—®é¢˜

```python
# æ£€æŸ¥ Chroma è¿æ¥
try:
    client = chromadb.CloudClient(
        api_key=API_KEY,
        tenant=TENANT,
        database=DATABASE
    )
    collections = client.list_collections()
    print(f"âœ… è¿æ¥æˆåŠŸï¼Œé›†åˆæ•°é‡: {len(collections)}")
except Exception as e:
    print(f"âŒ è¿æ¥å¤±è´¥: {e}")
```

### 2. åµŒå…¥æ¨¡å‹é—®é¢˜

```python
# æ£€æŸ¥æ¨¡å‹åŠ è½½
from sentence_transformers import SentenceTransformer

try:
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    test_embedding = model.encode(["æµ‹è¯•æ–‡æœ¬"])
    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼Œå‘é‡ç»´åº¦: {len(test_embedding[0])}")
except Exception as e:
    print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
```

### 3. æ–‡æ¡£å¤„ç†é—®é¢˜

```python
# æ£€æŸ¥æ–‡æ¡£åŠ è½½
from libs.knowledge_base.chroma_knowledge_base import DocumentLoader

loader = DocumentLoader()
try:
    doc = await loader.load_file("test.pdf")
    print(f"âœ… æ–‡æ¡£åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(doc.content)}")
except Exception as e:
    print(f"âŒ æ–‡æ¡£åŠ è½½å¤±è´¥: {e}")
```

## ğŸ“ˆ æ‰©å±•åŠŸèƒ½

### 1. è‡ªå®šä¹‰æ–‡æ¡£å¤„ç†å™¨

```python
class CustomLoader(BaseLoader):
    async def load(self, file_path: str) -> List[DocumentChunk]:
        # è‡ªå®šä¹‰åŠ è½½é€»è¾‘
        pass

# æ³¨å†Œè‡ªå®šä¹‰åŠ è½½å™¨
LoaderFactory.register_loader(".custom", CustomLoader)
```

### 2. é«˜çº§æœç´¢åŠŸèƒ½

```python
# æ··åˆæœç´¢ï¼ˆå‘é‡ + å…³é”®è¯ï¼‰
async def hybrid_search(query: str, **kwargs):
    # å‘é‡æœç´¢
    vector_results = await kb.search(query, **kwargs)
    
    # å…³é”®è¯æœç´¢
    keyword_results = await keyword_search(query, **kwargs)
    
    # ç»“æœèåˆ
    combined_results = merge_search_results(vector_results, keyword_results)
    return combined_results
```

### 3. å®æ—¶æ›´æ–°

```python
# æ–‡æ¡£å˜æ›´ç›‘å¬
import watchdog
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class DocumentWatcher(FileSystemEventHandler):
    def on_modified(self, event):
        if not event.is_directory:
            asyncio.create_task(kb.update_document(event.src_path))

# å¯åŠ¨æ–‡æ¡£ç›‘å¬
observer = Observer()
observer.schedule(DocumentWatcher(), "/path/to/documents", recursive=True)
observer.start()
```

## ğŸ“š æœ€ä½³å®è·µ

### 1. æ–‡æ¡£ç»„ç»‡

```
documents/
â”œâ”€â”€ ç•™å­¦æŒ‡å—/
â”‚   â”œâ”€â”€ ç”³è¯·æµç¨‹.pdf
â”‚   â”œâ”€â”€ GPAè®¡ç®—.md
â”‚   â””â”€â”€ æ¨èä¿¡æ¨¡æ¿.docx
â”œâ”€â”€ å­¦æ ¡ä¿¡æ¯/
â”‚   â”œâ”€â”€ ç¾å›½å¤§å­¦æ’å.txt
â”‚   â””â”€â”€ ä¸“ä¸šä»‹ç».html
â””â”€â”€ æ”¿ç­–æ³•è§„/
    â”œâ”€â”€ ç­¾è¯æ”¿ç­–.pdf
    â””â”€â”€ ç§»æ°‘æŒ‡å—.md
```

### 2. å…ƒæ•°æ®è®¾è®¡

```python
metadata = {
    "category": "ç•™å­¦æŒ‡å—",      # æ–‡æ¡£åˆ†ç±»
    "language": "ä¸­æ–‡",         # è¯­è¨€æ ‡è¯†  
    "source": "å®˜æ–¹ç½‘ç«™",       # æ¥æºä¿¡æ¯
    "version": "2024.1",       # ç‰ˆæœ¬å·
    "tags": ["GPA", "ç”³è¯·"],    # æ ‡ç­¾
    "author": "ç•™å­¦ä¸“å®¶",       # ä½œè€…
    "created_date": "2024-01-01",  # åˆ›å»ºæ—¥æœŸ
    "tenant_id": "user_123"    # ç§Ÿæˆ·ID
}
```

### 3. æŸ¥è¯¢ä¼˜åŒ–

```python
# æŸ¥è¯¢é¢„å¤„ç†
def preprocess_query(query: str) -> str:
    # å»é™¤åœç”¨è¯
    # åŒä¹‰è¯æ‰©å±•
    # æ‹¼å†™çº æ­£
    return processed_query

# ç»“æœåå¤„ç†
def postprocess_results(results: List[Dict]) -> List[Dict]:
    # å»é‡
    # é‡æ’åº
    # è´¨é‡è¿‡æ»¤
    return filtered_results
```

## ğŸ¯ å®é™…åº”ç”¨åœºæ™¯

### 1. ç•™å­¦å’¨è¯¢ Agent

```python
# ä¸“é—¨çš„ç•™å­¦å’¨è¯¢çŸ¥è¯†åº“
study_kb = ChromaKnowledgeBase(
    collection_name="study_abroad_guide",
    embedding_provider="sentence_transformers"
)

# æ·»åŠ ç•™å­¦ç›¸å…³æ–‡æ¡£
await study_kb.add_document("ç•™å­¦ç”³è¯·æŒ‡å—.pdf", metadata={"type": "guide"})
await study_kb.add_document("GPAè®¡ç®—æ–¹æ³•.md", metadata={"type": "reference"})
```

### 2. å¤šè¯­è¨€æ”¯æŒ

```python
# ä¸­æ–‡æ–‡æ¡£
await kb.add_document("ä¸­æ–‡æŒ‡å—.pdf", metadata={"language": "zh"})

# è‹±æ–‡æ–‡æ¡£  
await kb.add_document("english_guide.pdf", metadata={"language": "en"})

# æŒ‰è¯­è¨€æœç´¢
results = await kb.search("ç”³è¯·æµç¨‹", filter_metadata={"language": "zh"})
```

### 3. ç‰ˆæœ¬ç®¡ç†

```python
# æ·»åŠ ç‰ˆæœ¬åŒ–æ–‡æ¡£
await kb.add_document(
    "æ”¿ç­–æ–‡ä»¶.pdf",
    metadata={
        "version": "2024.1",
        "effective_date": "2024-01-01",
        "status": "current"
    }
)

# æŸ¥è¯¢æœ€æ–°ç‰ˆæœ¬
results = await kb.search(
    "ç­¾è¯æ”¿ç­–",
    filter_metadata={"status": "current"}
)
```

---

ğŸ’¡ **æç¤º**: çŸ¥è¯†åº“æ˜¯ AI Agent çš„é‡è¦ç»„æˆéƒ¨åˆ†ï¼Œå»ºè®®åœ¨å®é™…ä½¿ç”¨ä¸­æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´é…ç½®å‚æ•°å’Œå¤„ç†ç­–ç•¥ã€‚

